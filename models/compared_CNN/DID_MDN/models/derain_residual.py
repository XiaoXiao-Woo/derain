import torch
import torch.backends.cudnn as cudnn
import torch.nn as nn
import torch.nn.functional as F
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.model_zoo as model_zoo
from collections import OrderedDict
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.model_zoo as model_zoo
from collections import OrderedDict
import torchvision.models as models
from torch.autograd import Variable


def conv_block(in_dim, out_dim):
    return nn.Sequential(nn.Conv2d(in_dim, in_dim, kernel_size=3, stride=1, padding=1),
                         nn.ELU(True),
                         nn.Conv2d(in_dim, in_dim, kernel_size=3, stride=1, padding=1),
                         nn.ELU(True),
                         nn.Conv2d(in_dim, out_dim, kernel_size=1, stride=1, padding=0),
                         nn.AvgPool2d(kernel_size=2, stride=2))


def deconv_block(in_dim, out_dim):
    return nn.Sequential(nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),
                         nn.ELU(True),
                         nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),
                         nn.ELU(True),
                         nn.UpsamplingNearest2d(scale_factor=2))


def blockUNet1(in_c, out_c, name, transposed=False, bn=False, relu=True, dropout=False):
    block = nn.Sequential()
    if relu:
        block.add_module('%s.relu' % name, nn.ReLU(inplace=True))
    else:
        block.add_module('%s.leakyrelu' % name, nn.LeakyReLU(0.2, inplace=True))
    if not transposed:
        block.add_module('%s.conv' % name, nn.Conv2d(in_c, out_c, 3, 1, 1, bias=False))
    else:
        block.add_module('%s.tconv' % name, nn.ConvTranspose2d(in_c, out_c, 3, 1, 1, bias=False))
    if bn:
        block.add_module('%s.bn' % name, nn.BatchNorm2d(out_c))
    if dropout:
        block.add_module('%s.dropout' % name, nn.Dropout2d(0.5, inplace=True))
    return block


def blockUNet(in_c, out_c, name, transposed=False, bn=False, relu=True, dropout=False):
    block = nn.Sequential()
    if relu:
        block.add_module('%s.relu' % name, nn.ReLU(inplace=True))
    else:
        block.add_module('%s.leakyrelu' % name, nn.LeakyReLU(0.2, inplace=True))
    if not transposed:
        block.add_module('%s.conv' % name, nn.Conv2d(in_c, out_c, 4, 2, 1, bias=False))
    else:
        block.add_module('%s.tconv' % name, nn.ConvTranspose2d(in_c, out_c, 4, 2, 1, bias=False))
    if bn:
        block.add_module('%s.bn' % name, nn.BatchNorm2d(out_c))
    if dropout:
        block.add_module('%s.dropout' % name, nn.Dropout2d(0.5, inplace=True))
    return block


class D1(nn.Module):
    def __init__(self, nc, ndf, hidden_size):
        super(D1, self).__init__()

        # 256
        self.conv1 = nn.Sequential(nn.Conv2d(nc, ndf, kernel_size=3, stride=1, padding=1),
                                   nn.ELU(True))
        # 256
        self.conv2 = conv_block(ndf, ndf)
        # 128
        self.conv3 = conv_block(ndf, ndf * 2)
        # 64
        self.conv4 = conv_block(ndf * 2, ndf * 3)
        # 32
        self.encode = nn.Conv2d(ndf * 3, hidden_size, kernel_size=1, stride=1, padding=0)
        self.decode = nn.Conv2d(hidden_size, ndf, kernel_size=1, stride=1, padding=0)
        # 32
        self.deconv4 = deconv_block(ndf, ndf)
        # 64
        self.deconv3 = deconv_block(ndf, ndf)
        # 128
        self.deconv2 = deconv_block(ndf, ndf)
        # 256
        self.deconv1 = nn.Sequential(nn.Conv2d(ndf, ndf, kernel_size=3, stride=1, padding=1),
                                     nn.ELU(True),
                                     nn.Conv2d(ndf, ndf, kernel_size=3, stride=1, padding=1),
                                     nn.ELU(True),
                                     nn.Conv2d(ndf, nc, kernel_size=3, stride=1, padding=1),
                                     nn.Tanh())
        """
        self.deconv1 = nn.Sequential(nn.Conv2d(ndf,nc,kernel_size=3,stride=1,padding=1),
                                     nn.Tanh())
        """

    def forward(self, x):
        out1 = self.conv1(x)
        out2 = self.conv2(out1)
        out3 = self.conv3(out2)
        out4 = self.conv4(out3)
        out5 = self.encode(out4)
        dout5 = self.decode(out5)
        dout4 = self.deconv4(dout5)
        dout3 = self.deconv3(dout4)
        dout2 = self.deconv2(dout3)
        dout1 = self.deconv1(dout2)
        return dout1


class D(nn.Module):
    def __init__(self, nc, nf):
        super(D, self).__init__()

        main = nn.Sequential()
        # 256
        layer_idx = 1
        name = 'layer%d' % layer_idx
        main.add_module('%s.conv' % name, nn.Conv2d(nc, nf, 4, 2, 1, bias=False))

        # 128
        layer_idx += 1
        name = 'layer%d' % layer_idx
        main.add_module(name, blockUNet(nf, nf * 2, name, transposed=False, bn=True, relu=False, dropout=False))

        # 64
        layer_idx += 1
        name = 'layer%d' % layer_idx
        nf = nf * 2
        main.add_module(name, blockUNet(nf, nf * 2, name, transposed=False, bn=True, relu=False, dropout=False))

        # 32
        layer_idx += 1
        name = 'layer%d' % layer_idx
        nf = nf * 2
        main.add_module('%s.leakyrelu' % name, nn.LeakyReLU(0.2, inplace=True))
        main.add_module('%s.conv' % name, nn.Conv2d(nf, nf * 2, 4, 1, 1, bias=False))
        main.add_module('%s.bn' % name, nn.BatchNorm2d(nf * 2))

        # 31
        layer_idx += 1
        name = 'layer%d' % layer_idx
        nf = nf * 2
        main.add_module('%s.leakyrelu' % name, nn.LeakyReLU(0.2, inplace=True))
        main.add_module('%s.conv' % name, nn.Conv2d(nf, 1, 4, 1, 1, bias=False))
        main.add_module('%s.sigmoid' % name, nn.Sigmoid())
        # 30 (sizePatchGAN=30)

        self.main = main

    def forward(self, x):
        output = self.main(x)
        return output


class BottleneckBlock(nn.Module):
    def __init__(self, in_planes, out_planes, dropRate=0.0):
        super(BottleneckBlock, self).__init__()
        inter_planes = out_planes * 4
        self.bn1 = nn.BatchNorm2d(in_planes)
        # self.bn1.weight = self.bn1.weight.float()
        # self.bn1.bias = self.bn1.bias.float()
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(in_planes, inter_planes, kernel_size=1, stride=1,
                               padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(inter_planes)
        # self.bn2.weight = self.bn2.weight.float()
        # self.bn2.bias = self.bn2.bias.float()



        self.conv2 = nn.Conv2d(inter_planes, out_planes, kernel_size=3, stride=1,
                               padding=1, bias=False)
        self.droprate = dropRate

    def forward(self, x):
        x = x.float()
        out = self.conv1(self.relu(self.bn1(x)))
        if self.droprate > 0:
            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)
        out = self.conv2(self.relu(self.bn2(out)))
        if self.droprate > 0:
            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)
        return torch.cat([x, out], 1)


class BottleneckBlock1(nn.Module):
    def __init__(self, in_planes, out_planes, dropRate=0.0):
        super(BottleneckBlock1, self).__init__()
        inter_planes = out_planes * 4
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(in_planes, inter_planes, kernel_size=1, stride=1,
                               padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(inter_planes)
        self.conv2 = nn.Conv2d(inter_planes, out_planes, kernel_size=5, stride=1,
                               padding=2, bias=False)
        self.droprate = dropRate

    def forward(self, x):
        out = self.conv1(self.relu(self.bn1(x)))
        if self.droprate > 0:
            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)
        out = self.conv2(self.relu(self.bn2(out)))
        if self.droprate > 0:
            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)
        return torch.cat([x, out], 1)


class BottleneckBlock2(nn.Module):
    def __init__(self, in_planes, out_planes, dropRate=0.0):
        super(BottleneckBlock2, self).__init__()
        inter_planes = out_planes * 4
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(in_planes, inter_planes, kernel_size=1, stride=1,
                               padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(inter_planes)
        self.conv2 = nn.Conv2d(inter_planes, out_planes, kernel_size=7, stride=1,
                               padding=3, bias=False)
        self.droprate = dropRate

    def forward(self, x):
        out = self.conv1(self.relu(self.bn1(x)))
        if self.droprate > 0:
            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)
        out = self.conv2(self.relu(self.bn2(out)))
        if self.droprate > 0:
            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)
        return torch.cat([x, out], 1)


class TransitionBlock(nn.Module):
    def __init__(self, in_planes, out_planes, dropRate=0.0):
        super(TransitionBlock, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = nn.ConvTranspose2d(in_planes, out_planes, kernel_size=1, stride=1,
                                        padding=0, bias=False)
        self.droprate = dropRate

    def forward(self, x):
        out = self.conv1(self.relu(self.bn1(x)))
        if self.droprate > 0:
            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)
        return F.upsample_nearest(out, scale_factor=2)


class TransitionBlock1(nn.Module):
    def __init__(self, in_planes, out_planes, dropRate=0.0):
        super(TransitionBlock1, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = nn.ConvTranspose2d(in_planes, out_planes, kernel_size=1, stride=1,
                                        padding=0, bias=False)
        self.droprate = dropRate

    def forward(self, x):
        out = self.conv1(self.relu(self.bn1(x)))
        if self.droprate > 0:
            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)
        return F.avg_pool2d(out, 2)


class TransitionBlock3(nn.Module):
    def __init__(self, in_planes, out_planes, dropRate=0.0):
        super(TransitionBlock3, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = nn.ConvTranspose2d(in_planes, out_planes, kernel_size=1, stride=1,
                                        padding=0, bias=False)
        self.droprate = dropRate

    def forward(self, x):
        out = self.conv1(self.relu(self.bn1(x)))
        if self.droprate > 0:
            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)
        return out


class Dense_rain_residual(nn.Module):
    def __init__(self):
        super(Dense_rain_residual, self).__init__()

        self.dense_block1 = BottleneckBlock(3, 13)
        self.trans_block1 = TransitionBlock3(16, 8)

        ############# Block2-down 32-32  ##############
        self.dense_block2 = BottleneckBlock(8, 16)
        self.trans_block2 = TransitionBlock3(24, 16)

        ############# Block3-down  16-16 ##############
        self.dense_block3 = BottleneckBlock(16, 32)
        self.trans_block3 = TransitionBlock3(48, 16)

        ############# Block4-up  8-8  ##############
        self.dense_block4 = BottleneckBlock(16, 16)
        self.trans_block4 = TransitionBlock3(32, 16)

        ############# Block5-up  16-16 ##############
        self.dense_block5 = BottleneckBlock(16, 8)
        self.trans_block5 = TransitionBlock3(24, 8)

        self.dense_block6 = BottleneckBlock(8, 8)
        self.trans_block6 = TransitionBlock3(16, 4)

        # self.conv_refin=nn.Conv2d(9,20,3,1,1)
        self.conv_refin = nn.Conv2d(13, 20, 3, 1, 1)

        self.tanh = nn.Tanh()

        self.conv1010 = nn.Conv2d(20, 1, kernel_size=1, stride=1, padding=0)  # 1mm
        self.conv1020 = nn.Conv2d(20, 1, kernel_size=1, stride=1, padding=0)  # 1mm
        self.conv1030 = nn.Conv2d(20, 1, kernel_size=1, stride=1, padding=0)  # 1mm
        self.conv1040 = nn.Conv2d(20, 1, kernel_size=1, stride=1, padding=0)  # 1mm

        self.refine3 = nn.Conv2d(20 + 4, 3, kernel_size=3, stride=1, padding=1)
        # self.refine3= nn.Conv2d(20+4, 3, kernel_size=7,stride=1,padding=3)

        self.upsample = F.upsample_nearest

        self.relu = nn.LeakyReLU(0.2, inplace=True)

        self.batchnorm20 = nn.BatchNorm2d(20)
        self.batchnorm1 = nn.BatchNorm2d(1)

        self.dense0 = Dense_base_down0()
        self.dense1 = Dense_base_down1()
        self.dense2 = Dense_base_down2()

    def forward(self, x, label_d):
        ## 256x256

        # x = Variable(x.data, requires_grad=True)

        x3 = self.dense2(x)
        x2 = self.dense1(x)
        x1 = self.dense0(x)

        label_d11 = torch.FloatTensor(1)
        label_d11 = Variable(label_d11.cuda())
        shape_out = x3.data.size()
        sizePatchGAN = shape_out[3]

        # label_result=1
        label_result = float(label_d.data.cpu().float().numpy())

        label_d11.resize_((1, 4, sizePatchGAN, sizePatchGAN)).fill_(label_result)

        x8 = torch.cat([x1, x, x2, x3, label_d11], 1)
        # x8=torch.cat([x1,x,x2,x3],1)

        x9 = self.relu((self.conv_refin(x8)))

        shape_out = x9.data.size()
        # print(shape_out)
        shape_out = shape_out[2:4]

        x101 = F.avg_pool2d(x9, 32)
        x102 = F.avg_pool2d(x9, 16)
        x103 = F.avg_pool2d(x9, 8)
        x104 = F.avg_pool2d(x9, 4)

        x1010 = self.upsample(self.relu((self.conv1010(x101))), size=shape_out)
        x1020 = self.upsample(self.relu((self.conv1020(x102))), size=shape_out)
        x1030 = self.upsample(self.relu((self.conv1030(x103))), size=shape_out)
        x1040 = self.upsample(self.relu((self.conv1040(x104))), size=shape_out)

        dehaze = torch.cat((x1010, x1020, x1030, x1040, x9), 1)
        # dehaze = torch.cat((x1010, x1020, x1030, x1040), 1)

        residual = self.tanh(self.refine3(dehaze))
        return residual


class Dense_base_down2(nn.Module):
    def __init__(self):
        super(Dense_base_down2, self).__init__()

        self.dense_block1 = BottleneckBlock(3, 13)
        self.trans_block1 = TransitionBlock1(16, 8)

        ############# Block2-down 32-32  ##############
        self.dense_block2 = BottleneckBlock(8, 16)
        self.trans_block2 = TransitionBlock1(24, 16)

        ############# Block3-down  16-16 ##############
        self.dense_block3 = BottleneckBlock(16, 16)
        self.trans_block3 = TransitionBlock3(32, 16)

        ############# Block4-up  8-8  ##############
        self.dense_block4 = BottleneckBlock(16, 16)
        self.trans_block4 = TransitionBlock3(32, 16)

        ############# Block5-up  16-16 ##############
        self.dense_block5 = BottleneckBlock(16, 8)
        self.trans_block5 = TransitionBlock(24, 8)

        self.dense_block6 = BottleneckBlock(8, 8)
        self.trans_block6 = TransitionBlock(16, 2)

        self.conv_refin = nn.Conv2d(11, 20, 3, 1, 1)
        self.tanh = nn.Tanh()

        self.conv1010 = nn.Conv2d(20, 1, kernel_size=1, stride=1, padding=0)  # 1mm
        self.conv1020 = nn.Conv2d(20, 1, kernel_size=1, stride=1, padding=0)  # 1mm
        self.conv1030 = nn.Conv2d(20, 1, kernel_size=1, stride=1, padding=0)  # 1mm
        self.conv1040 = nn.Conv2d(20, 1, kernel_size=1, stride=1, padding=0)  # 1mm

        self.refine3 = nn.Conv2d(20 + 4, 3, kernel_size=3, stride=1, padding=1)
        # self.refine3= nn.Conv2d(20+4, 3, kernel_size=7,stride=1,padding=3)

        self.upsample = F.upsample_nearest

        self.relu = nn.LeakyReLU(0.2, inplace=True)

        self.batchnorm20 = nn.BatchNorm2d(20)
        self.batchnorm1 = nn.BatchNorm2d(1)

    def forward(self, x):
        ## 256x256
        x1 = self.dense_block1(x)
        x1 = self.trans_block1(x1)

        ###  32x32
        x2 = (self.dense_block2(x1))
        x2 = self.trans_block2(x2)

        # print x2.size()
        ### 16 X 16
        x3 = (self.dense_block3(x2))
        x3 = self.trans_block3(x3)

        ## Classifier  ##

        x4 = (self.dense_block4(x3))
        x4 = self.trans_block4(x4)

        x4 = x4 + x2

        x5 = (self.dense_block5(x4))
        x5 = self.trans_block5(x5)

        x5 = x5 + x1

        x6 = (self.dense_block6(x5))
        x6 = (self.trans_block6(x6))

        return x6


class Dense_base_down1(nn.Module):
    def __init__(self):
        super(Dense_base_down1, self).__init__()

        self.dense_block1 = BottleneckBlock(3, 13)
        self.trans_block1 = TransitionBlock1(16, 8)

        ############# Block2-down 32-32  ##############
        self.dense_block2 = BottleneckBlock(8, 16)
        self.trans_block2 = TransitionBlock3(24, 16)

        ############# Block3-down  16-16 ##############
        self.dense_block3 = BottleneckBlock(16, 16)
        self.trans_block3 = TransitionBlock3(32, 16)

        ############# Block4-up  8-8  ##############
        self.dense_block4 = BottleneckBlock(16, 16)
        self.trans_block4 = TransitionBlock3(32, 16)

        ############# Block5-up  16-16 ##############
        self.dense_block5 = BottleneckBlock(16, 8)
        self.trans_block5 = TransitionBlock3(24, 8)

        self.dense_block6 = BottleneckBlock(8, 8)
        self.trans_block6 = TransitionBlock(16, 2)

        self.conv_refin = nn.Conv2d(11, 20, 3, 1, 1)
        self.tanh = nn.Tanh()

        self.conv1010 = nn.Conv2d(20, 1, kernel_size=1, stride=1, padding=0)  # 1mm
        self.conv1020 = nn.Conv2d(20, 1, kernel_size=1, stride=1, padding=0)  # 1mm
        self.conv1030 = nn.Conv2d(20, 1, kernel_size=1, stride=1, padding=0)  # 1mm
        self.conv1040 = nn.Conv2d(20, 1, kernel_size=1, stride=1, padding=0)  # 1mm

        self.refine3 = nn.Conv2d(20 + 4, 3, kernel_size=3, stride=1, padding=1)
        # self.refine3= nn.Conv2d(20+4, 3, kernel_size=7,stride=1,padding=3)

        self.upsample = F.upsample_nearest

        self.relu = nn.LeakyReLU(0.2, inplace=True)

        self.batchnorm20 = nn.BatchNorm2d(20)
        self.batchnorm1 = nn.BatchNorm2d(1)

    def forward(self, x):
        ## 256x256
        x1 = self.dense_block1(x)
        x1 = self.trans_block1(x1)

        ###  32x32
        x2 = (self.dense_block2(x1))
        x2 = self.trans_block2(x2)

        # print x2.size()
        ### 16 X 16
        x3 = (self.dense_block3(x2))
        x3 = self.trans_block3(x3)

        ## Classifier  ##

        x4 = (self.dense_block4(x3))
        x4 = self.trans_block4(x4)

        x4 = x4 + x2

        x5 = (self.dense_block5(x4))
        x5 = self.trans_block5(x5)

        x5 = x5 + x1

        x6 = (self.dense_block6(x5))
        x6 = (self.trans_block6(x6))

        return x6


class Dense_base_down0(nn.Module):
    def __init__(self):
        super(Dense_base_down0, self).__init__()

        self.dense_block1 = BottleneckBlock(3, 5)
        self.trans_block1 = TransitionBlock3(8, 4)

        ############# Block2-down 32-32  ##############
        self.dense_block2 = BottleneckBlock(4, 8)
        self.trans_block2 = TransitionBlock3(12, 12)

        ############# Block3-down  16-16 ##############
        self.dense_block3 = BottleneckBlock(12, 4)
        self.trans_block3 = TransitionBlock3(16, 12)

        ############# Block4-up  8-8  ##############
        self.dense_block4 = BottleneckBlock(12, 4)
        self.trans_block4 = TransitionBlock3(16, 12)

        ############# Block5-up  16-16 ##############
        self.dense_block5 = BottleneckBlock(12, 8)
        self.trans_block5 = TransitionBlock3(20, 4)

        self.dense_block6 = BottleneckBlock(4, 8)
        self.trans_block6 = TransitionBlock3(12, 2)

    def forward(self, x):
        ## 256x256
        x1 = self.dense_block1(x)
        x1 = self.trans_block1(x1)

        ###  32x32
        x2 = (self.dense_block2(x1))
        x2 = self.trans_block2(x2)

        # print x2.size()
        ### 16 X 16
        x3 = (self.dense_block3(x2))
        x3 = self.trans_block3(x3)

        ## Classifier  ##

        x4 = (self.dense_block4(x3))
        x4 = self.trans_block4(x4)

        x4 = x4 + x2

        x5 = (self.dense_block5(x4))
        x5 = self.trans_block5(x5)

        x5 = x5 + x1

        x6 = (self.dense_block6(x5))
        x6 = (self.trans_block6(x6))

        return x6
